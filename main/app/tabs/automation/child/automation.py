import os
import sys
import shutil
import gradio as gr
from main.app.variables import translations, configs, index_path
from main.app.core.ui import gr_info, gr_warning, gr_error
from main.app.core.separate import separate_music
from main.app.core.training import preprocess, extract, create_index, training
from main.app.core.inference import convert_audio
from main.app.tabs.training.child.training import get_next_cos_name

def automation_workflow(
    training_files, 
    target_song, 
    model_name, 
    epochs,
    pitch_shift
):
    if not training_files:
        return None, "L·ªói: Ch∆∞a ch·ªçn file gi·ªçng h√°t ƒë·ªÉ train."
    if not target_song:
        return None, "L·ªói: Ch∆∞a ch·ªçn b√†i h√°t c·ªßa ca sƒ© ƒë·ªÉ ƒë·ªïi gi·ªçng."
    if not model_name:
        return None, "L·ªói: Ch∆∞a ƒë·∫∑t t√™n m√¥ h√¨nh."

    logs = []
    def log(msg):
        logs.append(msg)
        return "\n".join(logs)

    try:
        # =================================================================================
        # B∆Ø·ªöC 1: T√ÅCH GI·ªåNG TRAIN (DATASET)
        # =================================================================================
        yield None, log(f"== B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 1: T√ÅCH DATASET CHO MODEL {model_name} ==")
        
        # T·∫°o th∆∞ m·ª•c dataset t·∫°m th·ªùi
        dataset_dir = os.path.join("dataset", model_name)
        if os.path.exists(dataset_dir):
            shutil.rmtree(dataset_dir)
        os.makedirs(dataset_dir, exist_ok=True)

        # Di chuy·ªÉn file upload v√†o th∆∞ m·ª•c t·∫°m ƒë·ªÉ x·ª≠ l√Ω (n·∫øu c·∫ßn) ho·∫∑c d√πng tr·ª±c ti·∫øp
        # separate_music expects a list of file paths or a directory
        # V√¨ separate_music output ra structure ri√™ng, ta s·∫Ω d√πng output_dirs l√† dataset_dir
        # Tuy nhi√™n separate_music t·∫°o subfolder cho m·ªói b√†i h√°t. 
        # ƒê·ªÉ ƒë∆°n gi·∫£n cho training, ta c·∫ßn gom t·∫•t c·∫£ 'Vocals' v√†o 1 folder dataset model.
        
        # T√°ch t·ª´ng file m·ªôt v√† gom vocal
        dataset_train_ready = os.path.join("dataset", model_name) # ƒê√¢y l√† folder ch·ª©a wav 48k/32k s·∫°ch
        # Nh∆∞ng separate_music output ra subfolder.
        # Ta s·∫Ω t√°ch v√†o temp_separate tr∆∞·ªõc
        
        temp_separate_dir = os.path.join("audios", f"temp_train_{model_name}")
        stub_dir = os.path.join(temp_separate_dir, "stub")
        os.makedirs(stub_dir, exist_ok=True)
        
        # G·ªçi t√°ch nh·∫°c
        file_paths = [f.name for f in training_files]
        
        yield None, log(f"ƒêang t√°ch {len(file_paths)} file gi·ªçng train...")
        
        # Optimize: Skip instrumental denoising for training data
        os.environ["SKIP_INST_DENOISE"] = "1"
        
        separate_music(
            drop_audio_files=file_paths,
            input_path="",
            output_dirs=os.path.join(stub_dir, "stub"),
            export_format="wav",
            model_name="HP-Vocal-1",
            karaoke_model="", reverb_model="MDX-Reverb", denoise_model="Lite",
            sample_rate=44100, shifts=2, batch_size=1, overlap=0.25, aggression=10, 
            hop_length=1024, window_size=512, segments_size=256, post_process_threshold=0.2,
            enable_tta=False, enable_denoise=True, high_end_process=False, enable_post_process=False,
            separate_backing=False, separate_reverb=True # T√°ch reverb ƒë·ªÉ l·∫•y Original_Vocals_No_Reverb
        )
        
        # Gom Vocals v√†o dataset folder
        os.makedirs(dataset_train_ready, exist_ok=True)
        count_files = 0
        for root, dirs, files in os.walk(temp_separate_dir):
            for file in files:
                # Ch·ªâ l·∫•y file Original_Vocals_No_Reverb
                if "Original_Vocals_No_Reverb" in file and file.endswith(".wav"):
                    # Move and rename unique
                    src = os.path.join(root, file)
                    dst = os.path.join(dataset_train_ready, f"{count_files}.wav")
                    shutil.move(src, dst)
                    count_files += 1
        
        # D·ªçn d·∫πp temp
        shutil.rmtree(temp_separate_dir, ignore_errors=True)
        os.environ["SKIP_INST_DENOISE"] = "0"
        
        if count_files == 0:
             yield None, log(f"L·ªói: Kh√¥ng t√¨m th·∫•y file gi·ªçng t√°ch ƒë∆∞·ª£c trong {temp_separate_dir}. Vui l√≤ng ki·ªÉm tra l·∫°i log console.")
             return

        yield None, log(f"ƒê√£ chu·∫©n b·ªã xong d·ªØ li·ªáu train: {count_files} files.")

        # =================================================================================
        # B∆Ø·ªöC 2: HU·∫§N LUY·ªÜN M√î H√åNH
        # =================================================================================
        # =================================================================================
        # B∆Ø·ªöC 2: X·ª¨ L√ù B√ÄI H√ÅT ƒê√çCH (ƒê∆∞·ª£c ƒë∆∞a l√™n tr∆∞·ªõc Training)
        # =================================================================================
        yield None, log(f"== B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 2: X·ª¨ L√ù B√ÄI H√ÅT ƒê√çCH ==")
        
        target_path = target_song.name
        target_filename = os.path.splitext(os.path.basename(target_path))[0]
        output_target_dir = os.path.join("audios", target_filename)
        
        yield None, log(f"ƒêang t√°ch nh·∫°c b√†i: {target_filename}...")
        
        # T·∫°o stub cho audios ƒë·ªÉ workaround l·ªói c·∫Øt path
        audios_stub = os.path.join("audios", "stub")
        os.makedirs(audios_stub, exist_ok=True)

        separate_music(
            drop_audio_files=target_path,
            input_path="",
            output_dirs=os.path.join(audios_stub, "stub"), # separate_music creates subfolder automatically inside audios
            export_format="mp3",
            model_name="HP-Vocal-1",
            karaoke_model="", reverb_model="MDX-Reverb", denoise_model="Lite",
            sample_rate=44100, shifts=2, batch_size=1, overlap=0.25, aggression=10, 
            hop_length=1024, window_size=512, segments_size=256, post_process_threshold=0.2,
            enable_tta=False, enable_denoise=True, high_end_process=False, enable_post_process=False,
            separate_backing=False, separate_reverb=True # T√°ch reverb ƒë·ªÉ l·∫•y s·∫°ch
        )
        
        # T√¨m file Vocal v√† Instrument
        # separate output structure: audios/<filename>/...
        # File names: Original_Vocals_No_Reverb.mp3 (if dereverb), Instruments.mp3
        vocal_file = os.path.join(output_target_dir, "Original_Vocals_No_Reverb.mp3")
        if not os.path.exists(vocal_file):
             vocal_file = os.path.join(output_target_dir, "Original_Vocals.mp3")
        
        instrument_file = os.path.join(output_target_dir, "Instruments.mp3")
        
        if not os.path.exists(vocal_file) or not os.path.exists(instrument_file):
            yield None, log("L·ªói: Kh√¥ng t√¨m th·∫•y file t√°ch nh·∫°c (Vocal/Instrument).")
            return

        yield None, log("T√°ch nh·∫°c th√†nh c√¥ng.")

        # =================================================================================
        # B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN M√î H√åNH
        # =================================================================================
        yield None, log(f"== B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN ({epochs} epochs) ==")
        
        # Preprocess
        yield None, log("ƒêang ti·ªÅn x·ª≠ l√Ω...")
        for output in preprocess(
            model_name=model_name,
            dataset=dataset_train_ready,
            sample_rate="48k", # Default training sr
            cpu_core=os.cpu_count(),
            cut_preprocess="Automatic",
            process_effects=False, # Kh√¥ng effect
            clean_dataset=False, 
            clean_strength=0.7,
            chunk_len=3.0, overlap_len=0.3, normalization_mode="none"
        ):
            yield None, log(output)
        
        # Extract features
        yield None, log("ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (f0)...")
        for output in extract(
            model_name=model_name,
            version="v2",
            method="rmvpe",
            pitch_guidance=True,
            hop_length=160,
            cpu_cores=os.cpu_count(),
            gpu=0, # Auto detect usually logic handled inside
            sample_rate="48k",
            embedders="hubert_base",
            custom_embedders="",
            onnx_f0_mode=False,
            embedders_mode="fairseq",
            f0_autotune=False, f0_autotune_strength=1.0,
            hybrid_method="rmvpe", rms_extract=False, alpha=0.5
        ):
             yield None, log(output)
        
        # Create Index
        yield None, log("ƒêang t·∫°o ch·ªâ m·ª•c (index)...")
        for output in create_index(model_name, "v2", "Auto"):
             yield None, log(output)
        
        # Training
        yield None, log("ƒêang hu·∫•n luy·ªán (Training)... Vi·ªác n√†y c√≥ th·ªÉ m·∫•t th·ªùi gian.")
        for output in training(
            model_name=model_name,
            rvc_version="v2",
            save_every_epoch=50,
            save_only_latest=True,
            save_every_weights=True,
            total_epoch=epochs,
            sample_rate="48k",
            batch_size=8, # Safe default
            gpu=0,
            pitch_guidance=True,
            not_pretrain=False, # Use default pretrained
            custom_pretrained=False,
            pretrain_g="", 
            pretrain_d="",
            detector=False,
            threshold=50,
            clean_up=False,
            cache=True,
            model_author="", 
            vocoder="Default",
            checkpointing=False,
            deterministic=False, 
            benchmark=False, 
            optimizer="AdamW",
            energy_use=False,
            custom_reference=False, 
            reference_name="",
            multiscale_mel_loss=False
        ):
             yield None, log(output)
        yield None, log("Hu·∫•n luy·ªán ho√†n t·∫•t!")

        # =================================================================================
        # B∆Ø·ªöC 4: CHUY·ªÇN ƒê·ªîI V√Ä GH√âP (INFERENCE)
        # =================================================================================
        yield None, log(f"== B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 4: ƒê·ªîI GI·ªåNG V√Ä GH√âP NH·∫†C ==")
        
        # T√¨m file model .pth v√† .index
        model_pth = f"{model_name}.pth" # Usually resides in assets/weights
        
        # Helper t√¨m model file n·∫øu t√™n b·ªã ƒë·ªïi (v√≠ d·ª• th√™m s·ªë steps)
        weights_dir = os.path.join("assets", "weights")
        if not os.path.exists(os.path.join(weights_dir, model_pth)):
             chk_candidates = []
             if os.path.exists(weights_dir):
                 for f in os.listdir(weights_dir):
                     if f.startswith(model_name) and f.endswith(".pth"):
                         chk_candidates.append(f)
             
             if chk_candidates:
                 # Sort by name length or mtime? 
                 # Usually _latest suffix or _100e. sort by mtime is safer for "latest" run
                 chk_candidates.sort(key=lambda x: os.path.getmtime(os.path.join(weights_dir, x)), reverse=True)
                 model_pth = chk_candidates[0]
                 yield None, log(f"ƒê√£ t√¨m th·∫•y checkpoint model m·ªõi nh·∫•t: {model_pth}")
             else:
                 yield None, log(f"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file model kh·ªõp t√™n {model_pth} trong {weights_dir}")
        
        # T√¨m index file (V·ª´a t·∫°o ·ªü b∆∞·ªõc 2)
        # logs/<model_name>/added_...index
        index_file = ""
        logs_model_dir = os.path.join("assets", "logs", model_name)
        if os.path.exists(logs_model_dir):
            for f in os.listdir(logs_model_dir):
                if f.endswith(".index") and "added" in f:
                    index_file = os.path.join(logs_model_dir, f)
                    break
        
        # Output path
        final_output_path = os.path.join("audios", f"{target_filename}_COVER_{model_name}.mp3")
        
        yield None, log(f"ƒêang ƒë·ªïi gi·ªçng v√† gh√©p beat... (Model: {model_name})")
        
        # G·ªçi convert_audio
        # Params: clean, autotune, use_audio... large signature
        # We use explicit arguments based on convert.py signature
        result_paths = convert_audio(
            clean=True, clean_strength=0.5,
            autotune=False,
            use_audio=False, # Input directly path
            use_original=False, convert_backing=False, not_merge_backing=False, merge_instrument=False,
            pitch=pitch_shift,
            model=model_pth,
            index=index_file, index_rate=0.75,
            input=vocal_file,
            output=final_output_path,
            format="mp3",
            method="rmvpe", hybrid_method="rmvpe", hop_length=160,
            embedders="hubert_base", custom_embedders="",
            resample_sr=0, filter_radius=3, rms_mix_rate=0.25, protect=0.33,
            split_audio=False, f0_autotune_strength=1.0, input_audio_name="",
            checkpointing=False, onnx_f0_mode=False,
            formant_shifting=False, formant_qfrency=1.0, formant_timbre=1.0,
            f0_file=None, embedders_mode="fairseq",
            proposal_pitch=False, proposal_pitch_threshold=255.0,
            audio_processing=False, alpha=0.5,
            mix_beat=True, beat_file=instrument_file,
            mix_auto_gain=True,
            add_echo=True, echo_wet=0.25, echo_delay_ms=125
        )
        
        # convert_audio returns list. mix output is usually at index 6 based on convert.py
        # [vocal, backing, merge_back, original, merge_inst, update, mix_result]
        final_mix = result_paths[6]
        
        if final_mix and os.path.exists(final_mix):
             yield final_mix, log(f"== HO√ÄN T·∫§T! FILE K·∫æT QU·∫¢: {final_mix} ==")
        else:
             yield None, log("L·ªói: Kh√¥ng t·∫°o ƒë∆∞·ª£c file k·∫øt qu·∫£ cu·ªëi c√πng.")

    except Exception as e:
        import traceback
        err = traceback.format_exc()
        yield None, log(f"L·ªñI KH√îNG MONG MU·ªêN:\n{err}")


def automation_tab():
    with gr.Row():
        gr.Markdown("""
        # ü§ñ QUY TR√åNH T·ª∞ ƒê·ªòNG H√ìA (AUTO PIPELINE)
        Ch·ª©c nƒÉng n√†y s·∫Ω t·ª± ƒë·ªông ch·∫°y to√†n b·ªô quy tr√¨nh: 
        1. T√°ch gi·ªçng m·∫´u -> 2. T√°ch nh·∫°c ƒë√≠ch (Ca sƒ©) -> 3. Train Model -> 4. ƒê·ªïi gi·ªçng & Gh√©p nh·∫°c.
        """)
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### 1. D·ªØ Li·ªáu Hu·∫•n Luy·ªán (Gi·ªçng M·∫´u)")
            training_files = gr.Files(label="Ch·ªçn c√°c file gi·ªçng h√°t m·∫´u (WAV/MP3...)", file_types=["audio"])
            model_name = gr.Textbox(label="T√™n M√¥ H√¨nh (T·ª± ƒë·ªông t·∫°o)", value=get_next_cos_name(), interactive=True)
            epochs = gr.Slider(label="S·ªë v√≤ng hu·∫•n luy·ªán (Epochs)", minimum=10, maximum=1000, value=150, step=10)
        
        with gr.Column():
            gr.Markdown("### 2. B√†i H√°t C·∫ßn ƒê·ªïi Gi·ªçng")
            target_song = gr.File(label="Ch·ªçn b√†i h√°t c·ªßa ca sƒ© (WAV/MP3...)", file_types=["audio"])
            pitch_shift = gr.Slider(label="Ch·ªânh cao ƒë·ªô (Pitch)", minimum=-12, maximum=12, value=0, step=1, info="Nam -> N·ªØ: +12, N·ªØ -> Nam: -12")
            btn_run = gr.Button("üöÄ CH·∫†Y T·∫§T C·∫¢ (AUTO RUN)", variant="primary", scale=2)
    
    with gr.Row():
        logs = gr.Textbox(label="Nh·∫≠t k√Ω x·ª≠ l√Ω (Logs)", lines=15, interactive=False)
        output_audio = gr.Audio(label="K·∫æT QU·∫¢ CU·ªêI C√ôNG", interactive=False)

    btn_run.click(
        fn=automation_workflow,
        inputs=[training_files, target_song, model_name, epochs, pitch_shift],
        outputs=[output_audio, logs]
    )
