import os
import sys
import shutil
import librosa
import gradio as gr
from main.app.variables import translations, configs
from main.app.core.ui import gr_info, gr_warning, gr_error
from main.app.core.separate import separate_music
from main.app.core.training import preprocess, extract, create_index, training
from main.app.core.inference import convert_audio
from main.app.tabs.training.child.training import get_next_cos_name

def _ensure_dir(path: str) -> None:
    if path and not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def _pick_latest_model_file(model_name: str) -> str | None:
    weights_dir = configs.get("weights_path", os.path.join("assets", "weights"))
    if not os.path.exists(weights_dir):
        return None

    candidates = [
        f for f in os.listdir(weights_dir)
        if f.startswith(model_name) and f.endswith(".pth")
    ]
    if not candidates:
        return None

    candidates.sort(key=lambda f: os.path.getmtime(os.path.join(weights_dir, f)), reverse=True)
    return candidates[0]

def check_dataset_duration(dataset_dir: str) -> float:
    total_duration = 0.0
    if not os.path.exists(dataset_dir):
        return 0.0
    
    for root, _, files in os.walk(dataset_dir):
        for file in files:
            if file.endswith(".wav"):
                try:
                    path = os.path.join(root, file)
                    # Load audio without changing sr to keep it fast/accurate to file
                    y, sr = librosa.load(path, sr=None)
                    # Detect non-silent intervals (top_db=40 is a reasonable default for vocals)
                    intervals = librosa.effects.split(y, top_db=40)
                    # Calculate duration
                    non_silent_duration = sum((end - start) for start, end in intervals) / sr
                    total_duration += non_silent_duration
                except Exception as e:
                    print(f"Error checking duration for {file}: {e}")
                    
    return total_duration

def _pick_index_file(model_name: str) -> str:
    logs_dir = os.path.join(configs.get("logs_path", os.path.join("assets", "logs")), model_name)
    if not os.path.exists(logs_dir):
        return ""

    # ∆Øu ti√™n index ki·ªÉu "added_*.index"
    idx = [f for f in os.listdir(logs_dir) if f.endswith(".index") and "added" in f]
    if idx:
        idx.sort(key=lambda f: os.path.getmtime(os.path.join(logs_dir, f)), reverse=True)
        return os.path.join(logs_dir, idx[0])

    # Fallback: l·∫•y index m·ªõi nh·∫•t n·∫øu kh√¥ng c√≥ "added"
    all_idx = [f for f in os.listdir(logs_dir) if f.endswith(".index")]
    if not all_idx:
        return ""
    all_idx.sort(key=lambda f: os.path.getmtime(os.path.join(logs_dir, f)), reverse=True)
    return os.path.join(logs_dir, all_idx[0])

def automation_workflow(
    training_files, 
    target_song, 
    model_name, 
    epochs,
    pitch_shift,
    force_retrain
):
    if not training_files:
        return None, "L·ªói: Ch∆∞a ch·ªçn file gi·ªçng h√°t ƒë·ªÉ train."
    if not target_song:
        return None, "L·ªói: Ch∆∞a ch·ªçn b√†i h√°t c·ªßa ca sƒ© ƒë·ªÉ ƒë·ªïi gi·ªçng."
    if not model_name:
        return None, "L·ªói: Ch∆∞a ƒë·∫∑t t√™n m√¥ h√¨nh."

    logs = []
    def log(msg):
        logs.append(msg)
        return "\n".join(logs)

    try:
        # Check if model exists for Reuse Logic
        latest_model = _pick_latest_model_file(model_name)
        skip_training = False
        
        if latest_model:
            if force_retrain:
                yield None, log(f"Ph√°t hi·ªán model c≈© '{latest_model}' nh∆∞ng ch·∫°y l·∫°i theo y√™u c·∫ßu.")
            else:
                 yield None, log(f"Ph√°t hi·ªán model c≈© '{latest_model}'. B·ªè qua b∆∞·ªõc train.")
                 skip_training = True

        # =================================================================================
        # B∆Ø·ªöC 1: T√ÅCH GI·ªåNG TRAIN (DATASET)
        # =================================================================================
        if not skip_training:
            yield None, log(f"== B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 1: T√ÅCH DATASET CHO MODEL {model_name} ==")
        
            # T·∫°o th∆∞ m·ª•c dataset t·∫°m th·ªùi
            dataset_root = "dataset"
            dataset_dir = os.path.join(dataset_root, model_name)
            if os.path.exists(dataset_dir):
                shutil.rmtree(dataset_dir)
            os.makedirs(dataset_dir, exist_ok=True)

            # Di chuy·ªÉn file upload v√†o th∆∞ m·ª•c t·∫°m ƒë·ªÉ x·ª≠ l√Ω (n·∫øu c·∫ßn) ho·∫∑c d√πng tr·ª±c ti·∫øp
            # separate_music expects a list of file paths or a directory
            # V√¨ separate_music output ra structure ri√™ng, ta s·∫Ω d√πng output_dirs l√† dataset_dir
            # Tuy nhi√™n separate_music t·∫°o subfolder cho m·ªói b√†i h√°t. 
            # ƒê·ªÉ ƒë∆°n gi·∫£n cho training, ta c·∫ßn gom t·∫•t c·∫£ 'Vocals' v√†o 1 folder dataset model.
            
            # T√°ch t·ª´ng file m·ªôt v√† gom vocal
            dataset_train_ready = os.path.join(dataset_root, model_name) # ƒê√¢y l√† folder ch·ª©a wav s·∫°ch
            # Nh∆∞ng separate_music output ra subfolder.
            # Ta s·∫Ω t√°ch v√†o temp_separate tr∆∞·ªõc
            
            audios_root = configs.get("audios_path", "audios")
            temp_separate_dir = os.path.join(audios_root, f"temp_train_{model_name}")
            stub_dir = os.path.join(temp_separate_dir, "stub")
            os.makedirs(stub_dir, exist_ok=True)
            
            # G·ªçi t√°ch nh·∫°c
            file_paths = [f.name for f in training_files]
            
            yield None, log(f"ƒêang t√°ch {len(file_paths)} file gi·ªçng train...")
            
            # Optimize: Skip instrumental denoising for training data
            os.environ["SKIP_INST_DENOISE"] = "1"
            
            separate_music(
                drop_audio_files=file_paths,
                input_path="",
                output_dirs=os.path.join(stub_dir, "stub"),
                export_format="wav",
                model_name="HP-Vocal-1",
                karaoke_model="", reverb_model="MDX-Reverb", denoise_model="Lite",
                sample_rate=44100, shifts=2, batch_size=1, overlap=0.25, aggression=10, 
                hop_length=1024, window_size=512, segments_size=256, post_process_threshold=0.2,
                enable_tta=False, enable_denoise=True, high_end_process=False, enable_post_process=False,
                separate_backing=False, separate_reverb=True # T√°ch reverb ƒë·ªÉ l·∫•y Original_Vocals_No_Reverb
            )
            
            # Gom Vocals v√†o dataset folder
            os.makedirs(dataset_train_ready, exist_ok=True)
            count_files = 0
            for root, dirs, files in os.walk(temp_separate_dir):
                for file in files:
                    # Ch·ªâ l·∫•y file Original_Vocals_No_Reverb
                    if "Original_Vocals_No_Reverb" in file and file.endswith(".wav"):
                        # Move and rename unique
                        src = os.path.join(root, file)
                        dst = os.path.join(dataset_train_ready, f"{count_files}.wav")
                        shutil.move(src, dst)
                        count_files += 1
            
            # D·ªçn d·∫πp temp
            shutil.rmtree(temp_separate_dir, ignore_errors=True)
            os.environ["SKIP_INST_DENOISE"] = "0"
            
            if count_files == 0:
                 yield None, log(f"L·ªói: Kh√¥ng t√¨m th·∫•y file gi·ªçng t√°ch ƒë∆∞·ª£c trong {temp_separate_dir}. Vui l√≤ng ki·ªÉm tra l·∫°i log console.")
                 return

            # Quality Check: Duration (Silence Awareness)
            yield None, log(f"ƒêang ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu train (lo·∫°i b·ªè kho·∫£ng l·∫∑ng)...")
            effective_duration = check_dataset_duration(dataset_train_ready)
            if effective_duration < 60:
                 yield None, log(f"üõë L·ªñI: T·ªïng th·ªùi l∆∞·ª£ng gi·ªçng h√°t th·ª±c t·∫ø (ƒë√£ tr·ª´ kho·∫£ng l·∫∑ng) l√† {effective_duration:.2f}s.\n"
                                 f"H·ªá th·ªëng y√™u c·∫ßu t·ªëi thi·ªÉu 60s ƒë·ªÉ ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng model.\n"
                                 f"Vui l√≤ng th√™m file gi·ªçng h√°t ho·∫∑c d√πng file d√†i h∆°n.")
                 return
                 
            yield None, log(f"‚úÖ D·ªØ li·ªáu h·ª£p l·ªá: {count_files} files, Th·ªùi l∆∞·ª£ng th·ª±c t·∫ø: {effective_duration:.2f}s")
            yield None, log(f"ƒê√£ chu·∫©n b·ªã xong d·ªØ li·ªáu train.")
        else:
             # Need to define dataset_train_ready for Training step even if skipped?
             # Training step expects 'dataset_train_ready' as argument to preprocess.
             # But Step 3 is ALSO skipped if skip_training is True.
             # So we are good.
             yield None, log("‚è© B·ªè qua B∆∞·ªõc 1 (T√°ch dataset) v√¨ ƒëang d√πng l·∫°i model c≈©.")

        # =================================================================================
        # B∆Ø·ªöC 2: HU·∫§N LUY·ªÜN M√î H√åNH
        # =================================================================================
        # =================================================================================
        # B∆Ø·ªöC 2: X·ª¨ L√ù B√ÄI H√ÅT ƒê√çCH (ƒê∆∞·ª£c ƒë∆∞a l√™n tr∆∞·ªõc Training)
        # =================================================================================
        yield None, log(f"== B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 2: X·ª¨ L√ù B√ÄI H√ÅT ƒê√çCH ==")
        
        target_path = target_song.name
        target_filename = os.path.splitext(os.path.basename(target_path))[0]
        output_target_dir = os.path.join(audios_root, target_filename)
        
        yield None, log(f"ƒêang t√°ch nh·∫°c b√†i: {target_filename}...")
        
        # T·∫°o stub cho audios ƒë·ªÉ workaround l·ªói c·∫Øt path
        audios_stub = os.path.join(audios_root, "stub")
        os.makedirs(audios_stub, exist_ok=True)

        separate_music(
            drop_audio_files=target_path,
            input_path="",
            output_dirs=os.path.join(audios_stub, "stub"), # separate_music creates subfolder automatically inside audios
            export_format="mp3",
            model_name="HP-Vocal-1",
            karaoke_model="", reverb_model="MDX-Reverb", denoise_model="Lite",
            sample_rate=44100, shifts=2, batch_size=1, overlap=0.25, aggression=10, 
            hop_length=1024, window_size=512, segments_size=256, post_process_threshold=0.2,
            enable_tta=False, enable_denoise=True, high_end_process=False, enable_post_process=False,
            separate_backing=False, separate_reverb=True # T√°ch reverb ƒë·ªÉ l·∫•y s·∫°ch
        )
        
        # T√¨m file Vocal v√† Instrument
        # separate output structure: audios/<filename>/...
        # File names: Original_Vocals_No_Reverb.mp3 (if dereverb), Instruments.mp3
        vocal_file = os.path.join(output_target_dir, "Original_Vocals_No_Reverb.mp3")
        if not os.path.exists(vocal_file):
             vocal_file = os.path.join(output_target_dir, "Original_Vocals.mp3")
        
        instrument_file = os.path.join(output_target_dir, "Instruments.mp3")
        
        if not os.path.exists(vocal_file) or not os.path.exists(instrument_file):
            yield None, log("L·ªói: Kh√¥ng t√¨m th·∫•y file t√°ch nh·∫°c (Vocal/Instrument).")
            return

        yield None, log("T√°ch nh·∫°c th√†nh c√¥ng.")

        # =================================================================================
        # B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN M√î H√åNH
        # =================================================================================
        if not skip_training:
            yield None, log(f"== B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 3: HU·∫§N LUY·ªÜN ({epochs} epochs) ==")
        
            # Preprocess
            yield None, log("ƒêang ti·ªÅn x·ª≠ l√Ω...")
            for output in preprocess(
                model_name=model_name,
                dataset=dataset_train_ready, # Might be undefined if skipped, but this block is also skipped
                sample_rate="48k", # Default training sr
                cpu_core=os.cpu_count(),
                cut_preprocess="Automatic",
                process_effects=False, # Kh√¥ng effect
                clean_dataset=False, 
                clean_strength=0.7,
                chunk_len=3.0, overlap_len=0.3, normalization_mode="none"
            ):
                yield None, log(output)
            
            # Extract features
            yield None, log("ƒêang tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng (f0)...")
            for output in extract(
                model_name=model_name,
                version="v2",
                method="rmvpe",
                pitch_guidance=True,
                hop_length=160,
                cpu_cores=os.cpu_count(),
                gpu=0, # Auto detect usually logic handled inside
                sample_rate="48k",
                embedders="hubert_base",
                custom_embedders="",
                onnx_f0_mode=False,
                embedders_mode="fairseq",
                f0_autotune=False, f0_autotune_strength=1.0,
                hybrid_method="rmvpe", rms_extract=False, alpha=0.5
            ):
                 yield None, log(output)
            
            # Create Index
            yield None, log("ƒêang t·∫°o ch·ªâ m·ª•c (index)...")
            for output in create_index(model_name, "v2", "Auto"):
                 yield None, log(output)
            
            # Training
            yield None, log("ƒêang hu·∫•n luy·ªán (Training)... Vi·ªác n√†y c√≥ th·ªÉ m·∫•t th·ªùi gian.")
            for output in training(
                model_name=model_name,
                rvc_version="v2",
                save_every_epoch=50,
                save_only_latest=True,
                save_every_weights=True,
                total_epoch=epochs,
                sample_rate="48k",
                batch_size=8, # Safe default
                gpu=0,
                pitch_guidance=True,
                not_pretrain=False, # Use default pretrained
                custom_pretrained=False,
                pretrain_g="", 
                pretrain_d="",
                detector=False,
                threshold=50,
                clean_up=False,
                cache=True,
                model_author="", 
                vocoder="Default",
                checkpointing=False,
                deterministic=False, 
                benchmark=False, 
                optimizer="AdamW",
                energy_use=False,
                custom_reference=False, 
                reference_name="",
                multiscale_mel_loss=False
            ):
                yield None, log(output)
            yield None, log("Hu·∫•n luy·ªán ho√†n t·∫•t!")
        else:
             yield None, log("‚è© B·ªè qua B∆∞·ªõc 3 (Hu·∫•n luy·ªán) v√¨ ƒëang d√πng l·∫°i model c≈©.")

        # =================================================================================
        # B∆Ø·ªöC 4: CHUY·ªÇN ƒê·ªîI V√Ä GH√âP (INFERENCE)
        # =================================================================================
        yield None, log(f"== B·∫ÆT ƒê·∫¶U B∆Ø·ªöC 4: ƒê·ªîI GI·ªåNG V√Ä GH√âP NH·∫†C ==")
        
        # T√¨m model m·ªõi nh·∫•t trong weights (t√™n c√≥ th·ªÉ l√† cos02_150e_1800s.pth)
        model_pth = _pick_latest_model_file(model_name)
        if not model_pth:
            yield None, log(f"L·ªói: Kh√¥ng t√¨m th·∫•y model .pth trong `{configs.get('weights_path', 'assets/weights')}` cho `{model_name}`.")
            return
        yield None, log(f"ƒêang d√πng model: {model_pth}")

        # T√¨m index file m·ªõi nh·∫•t (∆∞u ti√™n added_*.index)
        index_file = _pick_index_file(model_name)
        if index_file:
            yield None, log(f"ƒêang d√πng index: {index_file}")
        else:
            yield None, log("C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file index, s·∫Ω ch·∫°y kh√¥ng d√πng index (ch·∫•t l∆∞·ª£ng c√≥ th·ªÉ k√©m h∆°n).")
        
        # Output path
        final_output_path = os.path.join(audios_root, f"{target_filename}_COVER_{model_name}.mp3")
        _ensure_dir(os.path.dirname(final_output_path))
        
        yield None, log(f"ƒêang ƒë·ªïi gi·ªçng v√† gh√©p beat... (Model: {model_name})")
        
        # G·ªçi convert_audio
        # Params: clean, autotune, use_audio... large signature
        # We use explicit arguments based on convert.py signature
        result_paths = convert_audio(
            clean=True, clean_strength=0.5,
            autotune=False,
            use_audio=False, # Input directly path
            use_original=False, convert_backing=False, not_merge_backing=False, merge_instrument=False,
            pitch=pitch_shift,
            model=model_pth,
            index=index_file, index_rate=0.75,
            input=vocal_file,
            output=final_output_path,
            format="mp3",
            method="rmvpe", hybrid_method="rmvpe", hop_length=160,
            embedders="hubert_base", custom_embedders="",
            resample_sr=0, filter_radius=3, rms_mix_rate=0.25, protect=0.33,
            split_audio=False, f0_autotune_strength=1.0, input_audio_name="",
            checkpointing=False, onnx_f0_mode=False,
            formant_shifting=False, formant_qfrency=1.0, formant_timbre=1.0,
            f0_file=None, embedders_mode="fairseq",
            proposal_pitch=False, proposal_pitch_threshold=255.0,
            audio_processing=False, alpha=0.5,
            mix_beat=True, beat_file=instrument_file,
            mix_auto_gain=True,
            add_echo=True, echo_wet=0.25, echo_delay_ms=125
        )
        
        if not result_paths or len(result_paths) < 7:
            yield None, log("L·ªói: convert_audio kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ h·ª£p l·ªá (c√≥ th·ªÉ do thi·∫øu model/ƒë·∫ßu v√†o/ƒë·∫ßu ra).")
            return

        # convert_audio returns list: [vocal, backing, merge_back, original, merge_inst, update, mix_result]
        final_mix = result_paths[6]
        
        if final_mix and os.path.exists(final_mix):
             yield final_mix, log(f"== HO√ÄN T·∫§T! FILE K·∫æT QU·∫¢: {final_mix} ==")
        else:
             # Log th√™m th√¥ng tin ƒë·ªÉ debug nhanh
             weights_dir = configs.get("weights_path", "assets/weights")
             logs_dir = configs.get("logs_path", "assets/logs")
             yield None, log(
                 "L·ªói: Kh√¥ng t·∫°o ƒë∆∞·ª£c file k·∫øt qu·∫£ cu·ªëi c√πng.\n"
                 f"- model_pth: {model_pth} (weights_dir={weights_dir})\n"
                 f"- index: {index_file or '(none)'} (logs_dir={logs_dir})\n"
                 f"- vocal_file: {vocal_file} (exists={os.path.exists(vocal_file)})\n"
                 f"- instrument_file: {instrument_file} (exists={os.path.exists(instrument_file)})\n"
                 f"- output: {final_output_path}"
             )

    except Exception as e:
        import traceback
        err = traceback.format_exc()
        yield None, log(f"L·ªñI KH√îNG MONG MU·ªêN:\n{err}")


def automation_tab():
    with gr.Row():
        gr.Markdown("""
        # ü§ñ QUY TR√åNH T·ª∞ ƒê·ªòNG H√ìA (AUTO PIPELINE)
        Ch·ª©c nƒÉng n√†y s·∫Ω t·ª± ƒë·ªông ch·∫°y to√†n b·ªô quy tr√¨nh: 
        1. T√°ch gi·ªçng m·∫´u -> 2. T√°ch nh·∫°c ƒë√≠ch (Ca sƒ©) -> 3. Train Model -> 4. ƒê·ªïi gi·ªçng & Gh√©p nh·∫°c.
        """)
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### 1. D·ªØ Li·ªáu Hu·∫•n Luy·ªán (Gi·ªçng M·∫´u)")
            training_files = gr.Files(label="Ch·ªçn c√°c file gi·ªçng h√°t m·∫´u (WAV/MP3...)", file_types=["audio"])
            model_name = gr.Textbox(label="T√™n M√¥ H√¨nh (T·ª± ƒë·ªông t·∫°o)", value=get_next_cos_name(), interactive=True)
            epochs = gr.Slider(label="S·ªë v√≤ng hu·∫•n luy·ªán (Epochs)", minimum=10, maximum=1000, value=150, step=10)
            force_retrain = gr.Checkbox(label="Hu·∫•n luy·ªán l·∫°i t·ª´ ƒë·∫ßu (B·ªè qua model c≈©)", value=False, info="T√≠ch v√†o ƒë√¢y n·∫øu b·∫°n mu·ªën train l·∫°i model n√†y thay v√¨ d√πng l·∫°i.")
        
        with gr.Column():
            gr.Markdown("### 2. B√†i H√°t C·∫ßn ƒê·ªïi Gi·ªçng")
            target_song = gr.File(label="Ch·ªçn b√†i h√°t c·ªßa ca sƒ© (WAV/MP3...)", file_types=["audio"])
            pitch_shift = gr.Slider(label="Ch·ªânh cao ƒë·ªô (Pitch)", minimum=-12, maximum=12, value=0, step=1, info="Nam -> N·ªØ: +12, N·ªØ -> Nam: -12")
            btn_run = gr.Button("üöÄ CH·∫†Y T·∫§T C·∫¢ (AUTO RUN)", variant="primary", scale=2)
    
    with gr.Row():
        logs = gr.Textbox(label="Nh·∫≠t k√Ω x·ª≠ l√Ω (Logs)", lines=15, interactive=False)
        output_audio = gr.Audio(label="K·∫æT QU·∫¢ CU·ªêI C√ôNG", interactive=False)

    btn_run.click(
        fn=automation_workflow,
        inputs=[training_files, target_song, model_name, epochs, pitch_shift, force_retrain],
        outputs=[output_audio, logs]
    )
